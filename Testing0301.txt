import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, GRU, Dense, Dropout
from tensorflow.keras.optimizers import Adam, RMSprop, SGD
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import numpy as np

# Sample Data (Replace with your dataset)
texts = ["I love machine learning", "Deep learning is fun", "I enjoy data science", 
         "Natural language processing is great", "Artificial intelligence is amazing"]
labels = ["tech", "tech", "data science", "NLP", "AI"]

# Parameters
MAX_NUM_WORDS = 10000  # Maximum number of words to consider in the tokenizer
MAX_SEQUENCE_LENGTH = 100  # Maximum length of input sequences
EMBEDDING_DIM = 100  # Dimensionality of the embedding layer
BATCH_SIZE = 32
EPOCHS = 10

# 1. Preprocessing the Text Data
# Tokenizing and padding the text data
tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)
tokenizer.fit_on_texts(texts)

X = tokenizer.texts_to_sequences(texts)
X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)

# 2. Label Encoding (using LabelEncoder)
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(labels)  # Integer encoding of the labels
y = to_categorical(y)  # Convert labels to one-hot encoded format for multi-class classification

# 3. Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 4. Grid Search Loop for LSTM Variants and Optimizers
lstm_variants = [
    LSTM(units=128, dropout=0.2, recurrent_dropout=0.2),  # Standard LSTM
    Bidirectional(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2)),  # Bidirectional LSTM
    LSTM(units=128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),  # Stacked LSTM (with return_sequences=True)
    GRU(units=128, dropout=0.2, recurrent_dropout=0.2),  # GRU Layer
    Bidirectional(GRU(units=128, dropout=0.2, recurrent_dropout=0.2)),  # Bidirectional GRU
    LSTM(units=128, dropout=0.3, recurrent_dropout=0.3),  # LSTM with higher dropout
    Bidirectional(LSTM(units=128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))  # Stacked Bidirectional LSTM
]

optimizers = [Adam(), RMSprop(), SGD()]

best_accuracy = 0
best_model = None
best_lstm_variant = None
best_optimizer = None

for lstm_variant in lstm_variants:
    for optimizer in optimizers:
        print(f"Training model with LSTM variant: {lstm_variant.__class__.__name__} and Optimizer: {optimizer.__class__.__name__}")

        # 5. Building the RNN Model
        model = Sequential()
        model.add(Embedding(input_dim=MAX_NUM_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))
        model.add(lstm_variant)
        model.add(Dense(units=y.shape[1], activation='softmax'))  # Softmax for multi-class classification

        # 6. Compile the Model
        model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])

        # 7. Train the Model
        history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_test, y_test), verbose=1)

        # 8. Evaluate the Model
        loss, accuracy = model.evaluate(X_test, y_test)
        print(f"Test Loss: {loss}")
        print(f"Test Accuracy: {accuracy}")

        if accuracy > best_accuracy:
            best_accuracy = accuracy
            best_model = model
            best_lstm_variant = lstm_variant
            best_optimizer = optimizer

# Best Model Evaluation
print(f"Best Model: LSTM Variant = {best_lstm_variant.__class__.__name__}, Optimizer = {best_optimizer.__class__.__name__}, Accuracy = {best_accuracy}")
