import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.metrics import make_scorer, silhouette_score
from sklearn.model_selection import GridSearchCV
from sklearn.feature_extraction.text import CountVectorizer
from gensim import corpora
from gensim.models import CoherenceModel
from sentence_transformers import SentenceTransformer
import matplotlib.pyplot as plt

# Sample data (you can replace this with your dataset)
documents = [
    "The stock market crashed today due to high inflation.",
    "A new AI model was released and it performs better than its predecessors.",
    "The weather today is sunny and perfect for a beach outing.",
    "Quantum computing is advancing rapidly with new breakthroughs.",
    "Inflation affects the purchasing power of individuals worldwide.",
    "The sun is bright and shining today.",
    "The weather forecast says it will rain tomorrow.",
    "The government is launching a new healthcare initiative.",
    "Machine learning and artificial intelligence are transforming industries.",
    "Data science involves statistics and machine learning.",
    "Healthcare reform is a priority for the government.",
    "The lake is calm and the weather is perfect for a swim.",
    "In technology, data-driven decisions lead to innovation.",
    "Financial markets are experiencing high volatility.",
    "The mountains are covered in snow."
]

# Step 1: Generate BERT Embeddings for Each Document
model = SentenceTransformer('bert-base-nli-mean-tokens')
embeddings = model.encode(documents)

# Step 2: Define Custom Scorer for Silhouette Score
def silhouette_scorer(estimator, X):
    cluster_labels = estimator.fit_predict(X)
    score = silhouette_score(X, cluster_labels)
    return score

# Step 3: Set Up Parameter Grid for KMeans
param_grid = {
    'n_clusters': [2, 3, 4, 5, 6],  # Range of clusters to try
    'init': ['k-means++', 'random'],  # Initialization methods
    'max_iter': [100, 200, 300]       # Number of iterations
}

# Step 4: Perform Grid Search with Custom Silhouette Score
kmeans = KMeans(random_state=0)
grid_search = GridSearchCV(
    kmeans,
    param_grid,
    scoring=make_scorer(silhouette_scorer),  # Custom scorer
    cv=[(slice(None), slice(None))],  # Dummy CV to work around GridSearchCV for unsupervised
    verbose=1
)
grid_search.fit(embeddings)

# Step 5: Get Best Parameters and Best Score
best_params = grid_search.best_params_
best_score = grid_search.best_score_
print(f"Best Parameters: {best_params}")
print(f"Best Silhouette Score: {best_score:.2f}")

# Step 6: Apply KMeans with Optimized Parameters
optimized_kmeans = KMeans(
    n_clusters=best_params['n_clusters'],
    init=best_params['init'],
    max_iter=best_params['max_iter'],
    random_state=0
)
kmeans_labels = optimized_kmeans.fit_predict(embeddings)

# Step 7: Calculate Final Silhouette Score
silhouette_kmeans = silhouette_score(embeddings, kmeans_labels)
print(f'Silhouette Score for Optimized KMeans: {silhouette_kmeans:.2f}')

# Step 8: Topic Coherence Calculation for KMeans Clustering
# Prepare the data for the Gensim dictionary
tokenized_documents = [doc.lower().split() for doc in documents]
gensim_dictionary = corpora.Dictionary(tokenized_documents)  # Gensim dictionary

# Extract top words per KMeans cluster
def get_top_words_per_cluster(labels, n_terms=10):
    cluster_keywords = []
    for cluster_num in range(best_params['n_clusters']):
        cluster_indices = np.where(labels == cluster_num)[0]
        cluster_docs = [documents[i] for i in cluster_indices]
        cluster_words = " ".join(cluster_docs).split()
        cluster_keywords.append(cluster_words[:n_terms])
    return cluster_keywords

top_words_per_cluster_kmeans = get_top_words_per_cluster(kmeans_labels)

# Create a coherence model
coherence_model = CoherenceModel(
    topics=top_words_per_cluster_kmeans,
    texts=tokenized_documents,
    dictionary=gensim_dictionary,
    coherence='c_v'
)
coherence_score = coherence_model.get_coherence()
print(f'Topic Coherence (C_V) for Optimized KMeans: {coherence_score:.2f}')

# Display Results
print("\nOptimized KMeans Cluster Results and Evaluations:")
for cluster_num in range(best_params['n_clusters']):
    cluster_indices = np.where(kmeans_labels == cluster_num)[0]
    print(f"\nCluster {cluster_num + 1}")
    print("Top Words:", ", ".join(top_words_per_cluster_kmeans[cluster_num]))
    for i in cluster_indices:
        print(f" - {documents[i]}")

print(f"\nSilhouette Score for Optimized KMeans: {silhouette_kmeans:.2f}")
print(f"Topic Coherence (C_V) for Optimized KMeans: {coherence_score:.2f}")
