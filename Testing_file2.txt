import numpy as np
import pandas as pd
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.metrics import silhouette_score
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import PCA
from gensim.models import CoherenceModel
from sentence_transformers import SentenceTransformer
import matplotlib.pyplot as plt

# Sample data (you can replace this with your dataset)
documents = [
    "The stock market crashed today due to high inflation.",
    "A new AI model was released and it performs better than its predecessors.",
    "The weather today is sunny and perfect for a beach outing.",
    "Quantum computing is advancing rapidly with new breakthroughs.",
    "Inflation affects the purchasing power of individuals worldwide.",
    "The sun is bright and shining today.",
    "The weather forecast says it will rain tomorrow.",
    "The government is launching a new healthcare initiative.",
    "Machine learning and artificial intelligence are transforming industries.",
    "Data science involves statistics and machine learning.",
    "Healthcare reform is a priority for the government.",
    "The lake is calm and the weather is perfect for a swim.",
    "In technology, data-driven decisions lead to innovation.",
    "Financial markets are experiencing high volatility.",
    "The mountains are covered in snow."
]

# Step 1: Generate BERT Embeddings for Each Document
model = SentenceTransformer('bert-base-nli-mean-tokens')
embeddings = model.encode(documents)

# Step 2: Apply Clustering Methods

# KMeans Clustering
num_clusters = 3  # Adjust based on your data
kmeans = KMeans(n_clusters=num_clusters, random_state=0)
kmeans_labels = kmeans.fit_predict(embeddings)

# DBSCAN Clustering
dbscan = DBSCAN(eps=1.0, min_samples=2)  # Tune 'eps' and 'min_samples' based on data
dbscan_labels = dbscan.fit_predict(embeddings)

# Agglomerative Clustering
agglo = AgglomerativeClustering(n_clusters=num_clusters)
agglo_labels = agglo.fit_predict(embeddings)

# Step 3: Calculate Silhouette Scores for Each Clustering Method
silhouette_kmeans = silhouette_score(embeddings, kmeans_labels)
silhouette_dbscan = silhouette_score(embeddings, dbscan_labels) if len(set(dbscan_labels)) > 1 else "N/A"  # Silhouette undefined if only one cluster
silhouette_agglo = silhouette_score(embeddings, agglo_labels)

print(f'Silhouette Score for KMeans: {silhouette_kmeans:.2f}')
print(f'Silhouette Score for DBSCAN: {silhouette_dbscan if silhouette_dbscan != "N/A" else "Not Applicable (only one cluster)"}')
print(f'Silhouette Score for Agglomerative Clustering: {silhouette_agglo:.2f}')

# Step 4: Topic Coherence Calculation for KMeans Clustering

# Convert each document to a bag of words representation
vectorizer = CountVectorizer(stop_words='english')
X = vectorizer.fit_transform(documents)
terms = vectorizer.get_feature_names_out()

# Extract top words per KMeans cluster
def get_top_words_per_cluster(labels, n_terms=10):
    cluster_keywords = []
    for cluster_num in range(num_clusters):
        cluster_indices = np.where(labels == cluster_num)[0]
        cluster_docs = [documents[i] for i in cluster_indices]
        cluster_words = " ".join(cluster_docs).split()
        cluster_keywords.append(cluster_words[:n_terms])
    return cluster_keywords

top_words_per_cluster_kmeans = get_top_words_per_cluster(kmeans_labels)

# Create a coherence model
coherence_model = CoherenceModel(
    topics=top_words_per_cluster_kmeans,
    texts=[doc.split() for doc in documents],
    dictionary=vectorizer.vocabulary_,
    coherence='c_v'
)
coherence_score = coherence_model.get_coherence()
print(f'Topic Coherence (C_V) for KMeans: {coherence_score:.2f}')

# Step 5: Visualize Clustering Results using PCA

# Reduce dimensionality for visualization
pca = PCA(n_components=2)
reduced_embeddings = pca.fit_transform(embeddings)

# Plotting function
def plot_clusters(reduced_embeddings, labels, title):
    plt.figure(figsize=(8, 5))
    unique_labels = set(labels)
    for label in unique_labels:
        if label == -1:  # DBSCAN noise points are labeled as -1
            color = 'k'  # black for noise
            marker = 'x'
        else:
            color = plt.cm.tab10(label / max(unique_labels))  # color map
            marker = 'o'
        
        plt.scatter(
            reduced_embeddings[labels == label, 0],
            reduced_embeddings[labels == label, 1],
            label=f"Cluster {label + 1}" if label != -1 else "Noise",
            color=color,
            marker=marker
        )
    plt.title(title)
    plt.xlabel("PCA Component 1")
    plt.ylabel("PCA Component 2")
    plt.legend()
    plt.show()

# Plot the clusters for each method
plot_clusters(reduced_embeddings, kmeans_labels, "KMeans Clustering")
plot_clusters(reduced_embeddings, dbscan_labels, "DBSCAN Clustering")
plot_clusters(reduced_embeddings, agglo_labels, "Agglomerative Clustering")

# Step 6: Display Results for KMeans Clustering
print("\nKMeans Cluster Results and Evaluations:")
for cluster_num in range(num_clusters):
    cluster_indices = np.where(kmeans_labels == cluster_num)[0]
    print(f"\nCluster {cluster_num + 1}")
    print("Top Words:", ", ".join(top_words_per_cluster_kmeans[cluster_num]))
    for i in cluster_indices:
        print(f" - {documents[i]}")

print(f"\nSilhouette Score for KMeans: {silhouette_kmeans:.2f}")
print(f"Topic Coherence (C_V) for KMeans: {coherence_score:.2f}")


import tensorflow_hub as hub
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Sample data
documents = [
    "The sun is bright and shining today.",
    "The weather forecast says it will rain tomorrow.",
    "The government is launching a new healthcare initiative.",
    "Machine learning and artificial intelligence are transforming industries.",
    "Data science involves statistics and machine learning.",
    "Healthcare reform is a priority for the government.",
    "The lake is calm and the weather is perfect for a swim.",
    "In technology, data-driven decisions lead to innovation.",
    "Financial markets are experiencing high volatility.",
    "The mountains are covered in snow."
]

# Load the Universal Sentence Encoder from TensorFlow Hub
embed = hub.load("https://tfhub.dev/google/universal-sentence-encoder/4")

# Step 1: Generate Embeddings for each document
embeddings = embed(documents)
embeddings_np = embeddings.numpy()  # Convert TensorFlow tensor to a numpy array

# Step 2: Apply Clustering on Embeddings

# KMeans Clustering
num_clusters = 3  # Set the desired number of clusters (topics)
kmeans = KMeans(n_clusters=num_clusters, random_state=0)
kmeans_labels = kmeans.fit_predict(embeddings_np)

# DBSCAN Clustering
dbscan = DBSCAN(eps=0.3, min_samples=2)
dbscan_labels = dbscan.fit_predict(embeddings_np)

# Agglomerative Clustering
agglo = AgglomerativeClustering(n_clusters=num_clusters)
agglo_labels = agglo.fit_predict(embeddings_np)

# Step 3: Visualize Clusters using PCA for Dimensionality Reduction

# Reduce embeddings to 2D for visualization
pca = PCA(n_components=2)
reduced_embeddings = pca.fit_transform(embeddings_np)

# Plotting function
def plot_clusters(reduced_embeddings, labels, title):
    plt.figure(figsize=(8, 5))
    unique_labels = set(labels)
    for label in unique_labels:
        color = plt.cm.tab10(label / max(unique_labels)) if label != -1 else 'k'  # black for noise
        marker = 'o' if label != -1 else 'x'
        plt.scatter(
            reduced_embeddings[labels == label, 0],
            reduced_embeddings[labels == label, 1],
            label=f"Cluster {label + 1}" if label != -1 else "Noise",
            color=color,
            marker=marker
        )
    plt.title(title)
    plt.xlabel("PCA Component 1")
    plt.ylabel("PCA Component 2")
    plt.legend()
    plt.show()

# Plot each clustering result
plot_clusters(reduced_embeddings, kmeans_labels, "KMeans Clustering with TensorFlow Embeddings")
plot_clusters(reduced_embeddings, dbscan_labels, "DBSCAN Clustering with TensorFlow Embeddings")
plot_clusters(reduced_embeddings, agglo_labels, "Agglomerative Clustering with TensorFlow Embeddings")



import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

# Sample data (you can replace this with your dataset)
documents = [
    "The sun is bright and shining today.",
    "The weather forecast says it will rain tomorrow.",
    "The government is launching a new healthcare initiative.",
    "Machine learning and artificial intelligence are transforming industries.",
    "Data science involves statistics and machine learning.",
    "Healthcare reform is a priority for the government.",
    "The lake is calm and the weather is perfect for a swim.",
    "In technology, data-driven decisions lead to innovation.",
    "Financial markets are experiencing high volatility.",
    "The mountains are covered in snow."
]

# Step 1: Vectorize the text data for LDA
vectorizer = CountVectorizer(stop_words='english')
doc_term_matrix = vectorizer.fit_transform(documents)

# Step 2: Apply LDA to extract topic distributions
num_topics = 3  # Number of topics to use in LDA
lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=0)
lda_topic_distributions = lda_model.fit_transform(doc_term_matrix)

# Step 3: Apply Clustering Methods on LDA topic distributions

# KMeans Clustering
kmeans = KMeans(n_clusters=num_topics, random_state=0)
kmeans_labels = kmeans.fit_predict(lda_topic_distributions)

# DBSCAN Clustering
dbscan = DBSCAN(eps=0.3, min_samples=2)  # Tune 'eps' and 'min_samples' based on data
dbscan_labels = dbscan.fit_predict(lda_topic_distributions)

# Agglomerative Clustering
agglo = AgglomerativeClustering(n_clusters=num_topics)
agglo_labels = agglo.fit_predict(lda_topic_distributions)

# Step 4: Visualize and Compare Results with PCA

# Reduce dimensionality for visualization
pca = PCA(n_components=2)
reduced_distributions = pca.fit_transform(lda_topic_distributions)

# Plotting function
def plot_clusters(reduced_distributions, labels, title):
    plt.figure(figsize=(8, 5))
    unique_labels = set(labels)
    for label in unique_labels:
        if label == -1:  # DBSCAN noise points are labeled as -1
            color = 'k'  # black for noise
            marker = 'x'
        else:
            color = plt.cm.tab10(label / max(unique_labels))  # color map
            marker = 'o'
        
        plt.scatter(
            reduced_distributions[labels == label, 0],
            reduced_distributions[labels == label, 1],
            label=f"Cluster {label + 1}" if label != -1 else "Noise",
            color=color,
            marker=marker
        )
    plt.title(title)
    plt.xlabel("PCA Component 1")
    plt.ylabel("PCA Component 2")
    plt.legend()
    plt.show()

# Plot the clusters for each method
plot_clusters(reduced_distributions, kmeans_labels, "KMeans Clustering with LDA")
plot_clusters(reduced_distributions, dbscan_labels, "DBSCAN Clustering with LDA")
plot_clusters(reduced_distributions, agglo_labels, "Agglomerative Clustering with LDA")






import numpy as np
import pandas as pd
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.metrics import silhouette_score
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import PCA
from gensim import corpora
from gensim.models import CoherenceModel
from sentence_transformers import SentenceTransformer
import matplotlib.pyplot as plt

# Sample data (you can replace this with your dataset)
documents = [
    "The stock market crashed today due to high inflation.",
    "A new AI model was released and it performs better than its predecessors.",
    "The weather today is sunny and perfect for a beach outing.",
    "Quantum computing is advancing rapidly with new breakthroughs.",
    "Inflation affects the purchasing power of individuals worldwide.",
    "The sun is bright and shining today.",
    "The weather forecast says it will rain tomorrow.",
    "The government is launching a new healthcare initiative.",
    "Machine learning and artificial intelligence are transforming industries.",
    "Data science involves statistics and machine learning.",
    "Healthcare reform is a priority for the government.",
    "The lake is calm and the weather is perfect for a swim.",
    "In technology, data-driven decisions lead to innovation.",
    "Financial markets are experiencing high volatility.",
    "The mountains are covered in snow."
]

# Step 1: Generate BERT Embeddings for Each Document
model = SentenceTransformer('bert-base-nli-mean-tokens')
embeddings = model.encode(documents)

# Step 2: Apply Clustering Methods

# KMeans Clustering
num_clusters = 3  # Adjust based on your data
kmeans = KMeans(n_clusters=num_clusters, random_state=0)
kmeans_labels = kmeans.fit_predict(embeddings)

# DBSCAN Clustering
dbscan = DBSCAN(eps=1.0, min_samples=2)  # Tune 'eps' and 'min_samples' based on data
dbscan_labels = dbscan.fit_predict(embeddings)

# Agglomerative Clustering
agglo = AgglomerativeClustering(n_clusters=num_clusters)
agglo_labels = agglo.fit_predict(embeddings)

# Step 3: Calculate Silhouette Scores for Each Clustering Method
silhouette_kmeans = silhouette_score(embeddings, kmeans_labels)
silhouette_dbscan = silhouette_score(embeddings, dbscan_labels) if len(set(dbscan_labels)) > 1 else "N/A"  # Silhouette undefined if only one cluster
silhouette_agglo = silhouette_score(embeddings, agglo_labels)

print(f'Silhouette Score for KMeans: {silhouette_kmeans:.2f}')
print(f'Silhouette Score for DBSCAN: {silhouette_dbscan if silhouette_dbscan != "N/A" else "Not Applicable (only one cluster)"}')
print(f'Silhouette Score for Agglomerative Clustering: {silhouette_agglo:.2f}')

# Step 4: Topic Coherence Calculation for KMeans Clustering

# Prepare the data for the Gensim dictionary
tokenized_documents = [doc.lower().split() for doc in documents]
gensim_dictionary = corpora.Dictionary(tokenized_documents)  # Gensim dictionary

# Extract top words per KMeans cluster
def get_top_words_per_cluster(labels, n_terms=10):
    cluster_keywords = []
    for cluster_num in range(num_clusters):
        cluster_indices = np.where(labels == cluster_num)[0]
        cluster_docs = [documents[i] for i in cluster_indices]
        cluster_words = " ".join(cluster_docs).split()
        cluster_keywords.append(cluster_words[:n_terms])
    return cluster_keywords

top_words_per_cluster_kmeans = get_top_words_per_cluster(kmeans_labels)

# Create a coherence model
coherence_model = CoherenceModel(
    topics=top_words_per_cluster_kmeans,
    texts=tokenized_documents,
    dictionary=gensim_dictionary,
    coherence='c_v'
)
coherence_score = coherence_model.get_coherence()
print(f'Topic Coherence (C_V) for KMeans: {coherence_score:.2f}')

# Step 5: Visualize Clustering Results using PCA

# Reduce dimensionality for visualization
pca = PCA(n_components=2)
reduced_embeddings = pca.fit_transform(embeddings)

# Plotting function
def plot_clusters(reduced_embeddings, labels, title):
    plt.figure(figsize=(8, 5))
    unique_labels = set(labels)
    for label in unique_labels:
        if label == -1:  # DBSCAN noise points are labeled as -1
            color = 'k'  # black for noise
            marker = 'x'
        else:
            color = plt.cm.tab10(label / max(unique_labels))  # color map
            marker = 'o'
        
        plt.scatter(
            reduced_embeddings[labels == label, 0],
            reduced_embeddings[labels == label, 1],
            label=f"Cluster {label + 1}" if label != -1 else "Noise",
            color=color,
            marker=marker
        )
    plt.title(title)
    plt.xlabel("PCA Component 1")
    plt.ylabel("PCA Component 2")
    plt.legend()
    plt.show()

# Plot the clusters for each method
plot_clusters(reduced_embeddings, kmeans_labels, "KMeans Clustering")
plot_clusters(reduced_embeddings, dbscan_labels, "DBSCAN Clustering")
plot_clusters(reduced_embeddings, agglo_labels, "Agglomerative Clustering")

# Step 6: Display Results for KMeans Clustering
print("\nKMeans Cluster Results and Evaluations:")
for cluster_num in range(num_clusters):
    cluster_indices = np.where(kmeans_labels == cluster_num)[0]
    print(f"\nCluster {cluster_num + 1}")
    print("Top Words:", ", ".join(top_words_per_cluster_kmeans[cluster_num]))
    for i in cluster_indices:
        print(f" - {documents[i]}")

print(f"\nSilhouette Score for KMeans: {silhouette_kmeans:.2f}")
print(f"Topic Coherence (C_V) for KMeans: {coherence_score:.2f}")







import numpy as np
import pandas as pd
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.metrics import silhouette_score
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import PCA
from gensim import corpora
from gensim.models import CoherenceModel
from sentence_transformers import SentenceTransformer
import matplotlib.pyplot as plt

# Sample data (you can replace this with your dataset)
documents = [
    "The stock market crashed today due to high inflation.",
    "A new AI model was released and it performs better than its predecessors.",
    "The weather today is sunny and perfect for a beach outing.",
    "Quantum computing is advancing rapidly with new breakthroughs.",
    "Inflation affects the purchasing power of individuals worldwide.",
    "The sun is bright and shining today.",
    "The weather forecast says it will rain tomorrow.",
    "The government is launching a new healthcare initiative.",
    "Machine learning and artificial intelligence are transforming industries.",
    "Data science involves statistics and machine learning.",
    "Healthcare reform is a priority for the government.",
    "The lake is calm and the weather is perfect for a swim.",
    "In technology, data-driven decisions lead to innovation.",
    "Financial markets are experiencing high volatility.",
    "The mountains are covered in snow."
]

# Step 1: Generate BERT Embeddings for Each Document
model = SentenceTransformer('bert-base-nli-mean-tokens')
embeddings = model.encode(documents)

# Step 2: Optimize Number of Clusters for KMeans using Elbow Method and Silhouette Score

# Variables to store scores for visualization
inertia = []
silhouette_scores = []
cluster_range = range(2, 11)  # Try clustering with 2 to 10 clusters

for num_clusters in cluster_range:
    kmeans = KMeans(n_clusters=num_clusters, random_state=0)
    kmeans_labels = kmeans.fit_predict(embeddings)
    
    # Append inertia (Sum of Squared Distances) and silhouette scores
    inertia.append(kmeans.inertia_)
    silhouette_avg = silhouette_score(embeddings, kmeans_labels)
    silhouette_scores.append(silhouette_avg)

# Plot Elbow Method
plt.figure(figsize=(10, 5))
plt.plot(cluster_range, inertia, 'bx-')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')
plt.title('Elbow Method for Optimal k')
plt.show()

# Plot Silhouette Scores
plt.figure(figsize=(10, 5))
plt.plot(cluster_range, silhouette_scores, 'bx-')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Score for Different k')
plt.show()

# Step 3: Choose Optimal Number of Clusters Based on the Elbow Method or Highest Silhouette Score
# Let's pick the best num_clusters based on the highest silhouette score
optimal_num_clusters = cluster_range[np.argmax(silhouette_scores)]
print(f'Optimal number of clusters based on silhouette score: {optimal_num_clusters}')

# Step 4: KMeans Clustering with Optimal Number of Clusters
kmeans = KMeans(n_clusters=optimal_num_clusters, random_state=0)
kmeans_labels = kmeans.fit_predict(embeddings)

# Step 5: Calculate Final Silhouette Score with Optimal Clusters
silhouette_kmeans = silhouette_score(embeddings, kmeans_labels)
print(f'Silhouette Score for Optimal KMeans (k={optimal_num_clusters}): {silhouette_kmeans:.2f}')

# Step 6: Topic Coherence Calculation for KMeans Clustering

# Prepare the data for the Gensim dictionary
tokenized_documents = [doc.lower().split() for doc in documents]
gensim_dictionary = corpora.Dictionary(tokenized_documents)  # Gensim dictionary

# Extract top words per KMeans cluster
def get_top_words_per_cluster(labels, n_terms=10):
    cluster_keywords = []
    for cluster_num in range(optimal_num_clusters):
        cluster_indices = np.where(labels == cluster_num)[0]
        cluster_docs = [documents[i] for i in cluster_indices]
        cluster_words = " ".join(cluster_docs).split()
        cluster_keywords.append(cluster_words[:n_terms])
    return cluster_keywords

top_words_per_cluster_kmeans = get_top_words_per_cluster(kmeans_labels)

# Create a coherence model
coherence_model = CoherenceModel(
    topics=top_words_per_cluster_kmeans,
    texts=tokenized_documents,
    dictionary=gensim_dictionary,
    coherence='c_v'
)
coherence_score = coherence_model.get_coherence()
print(f'Topic Coherence (C_V) for Optimal KMeans: {coherence_score:.2f}')

# Step 7: Visualize Clustering Results using PCA

# Reduce dimensionality for visualization
pca = PCA(n_components=2)
reduced_embeddings = pca.fit_transform(embeddings)

# Plotting function
def plot_clusters(reduced_embeddings, labels, title):
    plt.figure(figsize=(8, 5))
    unique_labels = set(labels)
    for label in unique_labels:
        color = plt.cm.tab10(label / max(unique_labels))  # color map
        plt.scatter(
            reduced_embeddings[labels == label, 0],
            reduced_embeddings[labels == label, 1],
            label=f"Cluster {label + 1}" if label != -1 else "Noise",
            color=color
        )
    plt.title(title)
    plt.xlabel("PCA Component 1")
    plt.ylabel("PCA Component 2")
    plt.legend()
    plt.show()

# Plot the clusters for KMeans with optimal number of clusters
plot_clusters(reduced_embeddings, kmeans_labels, f"KMeans Clustering (k={optimal_num_clusters})")

# Display Results for KMeans Clustering
print("\nKMeans Cluster Results and Evaluations:")
for cluster_num in range(optimal_num_clusters):
    cluster_indices = np.where(kmeans_labels == cluster_num)[0]
    print(f"\nCluster {cluster_num + 1}")
    print("Top Words:", ", ".join(top_words_per_cluster_kmeans[cluster_num]))
    for i in cluster_indices:
        print(f" - {documents[i]}")

print(f"\nSilhouette Score for Optimal KMeans: {silhouette_kmeans:.2f}")
print(f"Topic Coherence (C_V) for Optimal KMeans: {coherence_score:.2f}")
